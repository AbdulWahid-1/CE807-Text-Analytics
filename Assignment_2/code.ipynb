{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "iEU1n_FXBcdn",
        "outputId": "fc0816f8-e850-4371-ac89-d370aec1ccb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle \n",
        "import io \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n"
      ],
      "metadata": {
        "id": "X-s98BP_EVX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student ID: XXX\n",
        "\n",
        "**You student_id is your 7/8 digit faser number.** \n",
        "\n",
        "This is a sample format for CE807: Assignment 2. You must follow the format.\n",
        "The code will have three broad sections, and additional section, if needed,\n",
        "\n",
        "\n",
        "1.   Common Codes\n",
        "2.   Method/model 1 Specific Codes\n",
        "3.   Method/model 2 Specific Codes\n",
        "4.   Other Method/model Codes, if any\n",
        "\n",
        "**You must have `train_method1`, `test_method1` for method 1 and `train_method2`, `test_method2` for method 2 to perform full training and testing. This will be evaluated automatically, without this your code will fail and no marked.** \n",
        "\n",
        "You code should be proverly indended, print as much as possible, follow standard coding (https://peps.python.org/pep-0008/) and documentaion (https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.01-Help-And-Documentation.ipynb) practices. \n",
        "\n",
        "Before each `code cell`, you must have a `text cell` which explain what code cell is going to do. For each function/class, you need to properly document what are it's input, functionality and output. \n",
        "\n",
        "If you are using any non-standard library, you must have command to install that, for example `pip install datasets`. \n",
        "\n",
        "You must print `train`, `validation` and `test` performance measures.\n",
        "\n",
        "You must also print `train` and `validation` loss in each `epoch`, wherever you are using `epoch`, say in any deep learning algorithms.\n",
        "\n",
        "Your code must\n",
        "\n",
        "*   To reproducibality of the results you must use a `seed`, you have to set seed in `torch`, `numpy` etc, use same seed everywhere **and your Student ID should be your seed**. \n",
        "*   read dataset from './CE807/Assignment2/student_id/' folder which will have 3 files [`train.csv`, `val.csv`, `test.csv`]\n",
        "*   save model after finishing the training in './CE807/Assignment2/student_id/models/XXX/' where XXX = [1,2] for both models\n",
        "*   at testing time you will load models from './CE807/Assignment2/student_id/models/XXX/' where XXX = [1,2] and then test on your data, and save the output in the same folder\n",
        "*   For Data Size Effect, you model and output save directories are './CE807/Assignment2/student_id/models/XXX/YYY/' where XXX = [1,2] and YYY = [25,50, 75,100]\n",
        "*   **Your output file based on the test file will be named `output_test.csv` and will have fields `id`, `tweet`, `label` and `out_label`** Note that, `id`, `tweet`, `label` come from `test.csv`, and `out_label` out_label your model’s output, where out_label =[OFF,NOT]. You need to save file in the respective model folders. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Install and import all required libraries first before starting to code.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VgzEm1gDYBUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install all require libraries. For example, `transformers`"
      ],
      "metadata": {
        "id": "_3ZWJlO6JOqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "81A_4_dKJV4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa03901-b3ed-47a6-df80-65cac659747f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.0-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.2/224.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.0 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's put your student id as a variable, that you will use different places**"
      ],
      "metadata": {
        "id": "pd5kSsAPZoE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_id = 2202396 # Note this is an interger and you need to input your id"
      ],
      "metadata": {
        "id": "rqP6pp_3ZkVy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set `seed` for all libraries like `torch`, `numpy` etc as my student id"
      ],
      "metadata": {
        "id": "RiLUrQ-3zC6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set same seeds for all libraries\n",
        "\n",
        "#numpy seed\n",
        "np.random.seed(student_id)"
      ],
      "metadata": {
        "id": "TYUn2tj3zCFq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common Codes \n",
        "\n",
        "In this section you will write all common codes, for examples\n",
        "\n",
        "\n",
        "*   Data read\n",
        "*   Data Splitting\n",
        "*   Performance Matrics\n",
        "*   Print Dataset Statistics\n",
        "*   Saving model and output\n",
        "*   Loading Model and output\n",
        "*   etc\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dlj_VQrkbLgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's first allow the GDrive access and set data and model paths**\n",
        "\n",
        "For examples, \n",
        "\n",
        "student_id = 1234567\n",
        "\n",
        "set GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = ‘./CE807/Assignment2/student_id/’ in your GDrive\n",
        "\n",
        "now set all global variable, \n",
        "(Example for model 1 with 25% of data)\n",
        "\n",
        "train_file = os.path.join(GOOGLE_DRIVE_PATH_AFTER_MYDRIVE, 'train.csv')\n",
        "\n",
        "MODEL_1_DIRECTORY = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH, ‘models’, ‘1’)\n",
        "\n",
        "MODEL_1_25_DIRECTORY = os.path.join('MODEL_1_DIRECTORY',‘25’)\n",
        "\n",
        "model_1_25_output_test_file = os.path.join(MODEL_1_25_DIRECTORY,'output_test.csv')\n",
        "\n",
        "Sample output directory and file structure: https://drive.google.com/drive/folders/1okgSzgGiwPYYFp7NScEt9MNVolOlld1d?usp=share_link   "
      ],
      "metadata": {
        "id": "uOESFmIPn_nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "L1kvIe1NbDoS",
        "outputId": "ac9712d3-edd8-4a30-f728-f14ce56a84eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_1_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '1') # Model 1 directory\n",
        "print('Model 1 directory: ', MODEL_1_DIRECTORY)\n",
        "\n",
        "MODEL_1_25_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'25') # Model 1 trained using 25% of train data directory\n",
        "print('Model 1 directory with 25% data: ', MODEL_1_25_DIRECTORY)\n",
        "\n",
        "MODEL_1_100_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'100') # Model 1 trained using 25% of train data directory\n",
        "print('Model 1 directory with 100% data: ', MODEL_1_100_DIRECTORY)\n",
        "\n",
        "\n",
        "model_1_25_output_test_file = os.path.join(MODEL_1_25_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 25% of train data \n",
        "print('Output file name using model 1 using 25% of train data: ',model_1_25_output_test_file)\n",
        "\n",
        "model_1_100_output_test_file = os.path.join(MODEL_1_100_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 25% of train data \n",
        "print('Output file name using model 1 using 100% of train data: ',model_1_100_output_test_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WKnbP3roLTj",
        "outputId": "38ee3778-3619-4054-d32d-f3eef07bb030"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1 directory:  gdrive/MyDrive/./CE807/Assignment2/2202396/models/1\n",
            "Model 1 directory with 25% data:  gdrive/MyDrive/./CE807/Assignment2/2202396/models/1/25\n",
            "Model 1 directory with 100% data:  gdrive/MyDrive/./CE807/Assignment2/2202396/models/1/100\n",
            "Output file name using model 1 using 25% of train data:  gdrive/MyDrive/./CE807/Assignment2/2202396/models/1/25/output_test.csv\n",
            "Output file name using model 1 using 100% of train data:  gdrive/MyDrive/./CE807/Assignment2/2202396/models/1/100/output_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see train file"
      ],
      "metadata": {
        "id": "0pWvXDghtafa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cleaning Data set**"
      ],
      "metadata": {
        "id": "zLhyzYpxzh7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data into a pandas DataFrame\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/CE807/Assignment2/2202396/train.csv')\n",
        "\n",
        "# Removing duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Checking for missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "df.to_csv('/content/gdrive/MyDrive/CE807/Assignment2/2202396/cleaned_data.csv', index=False)"
      ],
      "metadata": {
        "id": "NeVs4XmiybKC",
        "outputId": "08b35519-5694-4ae4-fd11-9ab9fb7e44b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id       0\n",
            "tweet    0\n",
            "label    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use different performance matrics like Accuracy, Recall (macro), Precision (macro), F1 (macro) and Confusion Matrix for the performance evaluation. We will print all the matrics and display Confusion Matrix with proper X & Y axis labels"
      ],
      "metadata": {
        "id": "E-OjJ4REbhcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_performance(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    prints different performance matrics like  Accuracy, Recall (macro), Precision (macro), and F1 (macro).\n",
        "    This also display Confusion Matrix with proper X & Y axis labels.\n",
        "    Also, returns F1 score\n",
        "\n",
        "    Args:\n",
        "        y_true: numpy array or list\n",
        "        y_pred: numpy array or list\n",
        "        \n",
        "\n",
        "    Returns:\n",
        "        float\n",
        "    \"\"\"\n",
        "\n",
        "    ##########################################################################\n",
        "    #                     TODO: Implement this function                      #\n",
        "    ##########################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    # Evaluate the model performance using metrics such as accuracy, precision, recall, and F1-score\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, pos_label='offensive')\n",
        "    recall = recall_score(y_test, y_pred, pos_label='offensive')\n",
        "    f1 = f1_score(y_test, y_pred, pos_label='offensive')\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-score:\", f1)\n",
        "    ##########################################################################\n",
        "    #                            END OF YOUR CODE                            #\n",
        "    ##########################################################################\n",
        "    return f1_score"
      ],
      "metadata": {
        "id": "aLuUu5BWcTid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_performance(y_true, y_pred, split='test'):\n",
        "    \"\"\"\n",
        "    prints different performance matrics like  Accuracy, Recall (macro), Precision (macro), and F1 (macro).\n",
        "    This also display Confusion Matrix with proper X & Y axis labels.\n",
        "    Also, returns F1 score\n",
        "\n",
        "    Args:\n",
        "        y_true: numpy array or list\n",
        "        y_pred: numpy array or list\n",
        "        split: str\n",
        "        \n",
        "\n",
        "    Returns:\n",
        "        float\n",
        "    \"\"\"\n",
        "\n",
        "    print('Computing different preformance metrics on', split, ' set of Dataset')\n",
        "    f1score=f1_score(y_true, y_pred, average='macro')\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    \n",
        "    print('F1 Score(macro): ', f1score)\n",
        "    print('Accuracy: ', acc)\n",
        "\n",
        "    return f1score"
      ],
      "metadata": {
        "id": "jjpjwhcKG6O-"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1 Start\n",
        "\n",
        "In this section you will write all details of your Method 1. \n",
        "\n",
        "You will have to enter multiple `code` and `text` cell.\n",
        "\n",
        "Your code should follow the standard ML pipeline\n",
        "\n",
        "\n",
        "*   Data reading\n",
        "*   Data clearning, if any\n",
        "*   Convert data to vector/tokenization/vectorization\n",
        "*   Model Declaration/Initialization/building\n",
        "*   Training and validation of the model using training and validation dataset \n",
        "*   Save the trained model\n",
        "*   Load and Test the model on testing set\n",
        "*   Save the output of the model\n",
        "\n",
        "\n",
        "You could add any other step(s) based on your method's requirement. \n",
        "\n",
        "After finishing the above, you need to usd splited data as defined in the assignment and then do the same for all 4 sets. Your code should not be copy-pasted 4 time, make use of `function`.\n"
      ],
      "metadata": {
        "id": "47ywe8jGSKhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset1(data, count_vectorizer=None, split='test'):\n",
        "  if split == 'train':\n",
        "      count_vectorizer = CountVectorizer(stop_words='english',max_features=5000) \n",
        "      values = count_vectorizer.fit_transform(data['tweet'].values) #TODO: This is the best way to do this, because you need to use same vectorization menthod\n",
        "  else:\n",
        "      values = count_vectorizer.transform(data['tweet'].values)\n",
        "\n",
        "  if split == 'train':\n",
        "      return values, count_vectorizer\n",
        "  else:\n",
        "      return values"
      ],
      "metadata": {
        "id": "Z-bthsb3SX6o"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model1(text_vector,label):\n",
        "\n",
        "    print('Let\\'s start training RandomForestClassifier')\n",
        "    classifier = RandomForestClassifier() \n",
        "    classifier.fit(text_vector, label)\n",
        "\n",
        "    return classifier"
      ],
      "metadata": {
        "id": "33AJ3hZbHC18"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8MUZRNYRhUDI",
        "outputId": "a84ad176-64e8-42ea-f55a-c0b9daa7e6d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model1(model, vectorizer, model_dir):\n",
        "    # save the model to disk\n",
        "    model_file = os.path.join(model_dir, 'model.sav')\n",
        "    pickle.dump(model, open(model_file, 'wb'))\n",
        "\n",
        "    print('Saved model to ', model_file)\n",
        "\n",
        "    vectorizer_file = os.path.join(model_dir, 'vectorizer.sav') \n",
        "    pickle.dump(vectorizer, open(vectorizer_file, 'wb'))\n",
        "\n",
        "    print('Saved Vectorizer to ', vectorizer_file)\n",
        "\n",
        "    return model_file, vectorizer_file "
      ],
      "metadata": {
        "id": "por7XIQ_HMhm"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model1(model_file, vectorizer_file):\n",
        "    # load model and vectorizer from disk\n",
        "\n",
        "    model = pickle.load(open(model_file, 'rb'))\n",
        "\n",
        "    print('Loaded model from ', model_file)\n",
        "\n",
        "    vectorizer = pickle.load(open(vectorizer_file, 'rb'))\n",
        "\n",
        "    print('Loaded Vectorizer from ', vectorizer_file)\n",
        "\n",
        "\n",
        "    return model, vectorizer"
      ],
      "metadata": {
        "id": "6wxAPnClHPzk"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Method 1 Code\n",
        "Your test code should be a stand alone code that must take `train_file`, `val_file`,  and `model_dir` as input. You could have other things as also input, but these three are must. You would load both files, and train using the `train_file` and validating using the `val_file`. You will `print` / `display`/ `plot` all performance metrics, loss(if available) and save the output model in the `model_dir`.\n",
        "\n",
        "Note that at the testing time, you need to use the same pre-processing and model. So, it would be good that you make those as seperate function/pipeline whichever it the best suited for your method. Don't copy-paste same code twice, make it a fucntion/class whichever is best. "
      ],
      "metadata": {
        "id": "1sA3OWlVbnoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "pkcr7pc42XZE"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_method1(train_file, val_file, model_dir):\n",
        "    \"\"\"\n",
        "     Takes train_file, val_file and model_dir as input.\n",
        "     It trained on the train_file datapoints, and validate on the val_file datapoints.\n",
        "     While training and validating, it print different evaluataion metrics and losses, wheverever necessary.\n",
        "     After finishing the training, it saved the best model in the model_dir.\n",
        "\n",
        "     ADD Other arguments, if needed.\n",
        "\n",
        "    Args:\n",
        "        train_file: Train file name\n",
        "        val_file: Validation file name\n",
        "        model_dir: Model output Directory\n",
        "    \n",
        "    \"\"\"\n",
        "    train_df = pd.read_csv(train_file)\n",
        "    val_df = pd.read_csv(val_file)\n",
        "\n",
        "    train_label = train_df['label']\n",
        "    val_label = val_df['label']\n",
        "\n",
        "    train_values, count_vectorizer = prepare_dataset1(train_df, split='train') \n",
        "    val_values= prepare_dataset1(val_df,count_vectorizer)\n",
        "\n",
        "    model = train_model1(train_values,train_label)\n",
        "\n",
        "    model_file, vectorizer_file = save_model1(model, count_vectorizer, model_dir)\n",
        "\n",
        "    train_pred_label = model.predict(train_values)\n",
        "    val_pred_label = model.predict(val_values)\n",
        "\n",
        "    # print('Train Split')\n",
        "    train_f1_score = compute_performance(train_label, train_pred_label, split='train')\n",
        "\n",
        "    # print('Validation Split')\n",
        "    val_f1_score = compute_performance(val_label, val_pred_label, split='valid')\n",
        "\n",
        "\n",
        "    return model_file, vectorizer_file"
      ],
      "metadata": {
        "id": "_Ux7GBKXbqq6"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv') # This is 100% of data\n",
        "train_25_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_25.csv') #Let's assume that you have train 25% file is saved in train_25.csv. Note that this is a dummy file. You have to create your own file.\n",
        "\n",
        "\n",
        "print('Train 100% file: ', train_file)\n",
        "print('Train 25% file: ', train_25_file)\n",
        "\n",
        "val_file = os.path.join(GOOGLE_DRIVE_PATH, 'valid.csv')\n",
        "print('Validation file: ', val_file)\n",
        "\n",
        "test_file = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')\n",
        "print('Test file: ', test_file)"
      ],
      "metadata": {
        "id": "CKyCYpbRIdQh",
        "outputId": "bd929ab1-6298-4d75-e363-a81510b7f29c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 100% file:  gdrive/MyDrive/./CE807/Assignment2/2202396/train.csv\n",
            "Train 25% file:  gdrive/MyDrive/./CE807/Assignment2/2202396/train_25.csv\n",
            "Validation file:  gdrive/MyDrive/./CE807/Assignment2/2202396/valid.csv\n",
            "Test file:  gdrive/MyDrive/./CE807/Assignment2/2202396/test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train using of 100% of data')\n",
        "model_100_file, vectorizer_100_file = train_method1(train_file, val_file, MODEL_1_100_DIRECTORY)"
      ],
      "metadata": {
        "id": "KU0urEAwHb3t",
        "outputId": "93892832-d6c3-465f-d820-7eab375f6883",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 100% of data\n",
            "Let's start training MultinomialNB\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2202396/models/1/100/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2202396/models/1/100/vectorizer.sav\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.9933904329025784\n",
            "Accuracy:  0.9941525217250061\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.7216294342164347\n",
            "Accuracy:  0.7691477885652643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_method1(train_df, valid_df, model_dir)"
      ],
      "metadata": {
        "id": "kwSh1LMmAMYs",
        "outputId": "6da23189-5e68-409a-e41e-93382bb1c581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-a5697ab56809>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_method1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-60-86fbfeb8d065>\u001b[0m in \u001b[0;36mtrain_method1\u001b[0;34m(train_file, val_file, model_dir)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Train a Naive Bayes classifier on the vectorized data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Predict the labels for the testing set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \"\"\"\n\u001b[1;32m    748\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(self, X, y, reset)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;34m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_class_log_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3, 12313]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define a function to train and save the model\n",
        "def train_and_save_model(x_train, y_train, model_path):\n",
        "    # Define the model\n",
        "    model = LogisticRegression()\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    # Save the model\n",
        "    with open(model_path, 'wb') as file:\n",
        "        pickle.dump(model, file)\n",
        "\n",
        "# Define a function to load the saved model and make predictions\n",
        "def load_and_predict(X_test, model_path):\n",
        "    # Load the model\n",
        "    with open(model_path, 'rb') as file:\n",
        "        model = pickle.load(file)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "NwxZPpZ_CVvy"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_save_model(train_df, valid_df, model_dir)"
      ],
      "metadata": {
        "id": "jMjEYXj9Cesm",
        "outputId": "cc95ec7c-b3c2-490b-8a28-2276ef6e5a36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-2ea3cdf439cd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-5147d57d74b0>\u001b[0m in \u001b[0;36mtrain_and_save_model\u001b[0;34m(x_train, y_train, model_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1194\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m   1197\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"numpy.array_api\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2070\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m     def __array_wrap__(\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '@USER I’m done with you as well. An INTENTIONAL 11th hour attack completely based on hearsay and you want an investigation? What do you think they’ll find other than he said she said. Feinstein admitted she sat on this and here you are ready to lockstep with the swamp.'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Method 1 Code\n",
        "Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`  "
      ],
      "metadata": {
        "id": "qyJ_xv12Uy9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trained model to predict the label for new, unseen text data\n",
        "new_text = \"This is an offensive sentence.\"\n",
        "new_tokens = word_tokenize(new_text)\n",
        "new_vec = vectorizer.transform([new_tokens])\n",
        "new_pred = clf.predict(new_vec)\n",
        "print(\"Predicted label:\", new_pred[0])"
      ],
      "metadata": {
        "id": "Gb9R9n0U1acp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_method1(test_file, model_file, output_dir):\n",
        "    \"\"\"\n",
        "     take test_file, model_file and output_dir as input.\n",
        "     It loads model and test of the examples in the test_file.\n",
        "     It prints different evaluation metrics, and saves the output in output directory\n",
        "\n",
        "     ADD Other arguments, if needed\n",
        "\n",
        "    Args:\n",
        "        test_file: test file name\n",
        "        model_file: model file name\n",
        "        output_dir: Output Directory\n",
        "        \n",
        "\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    ##########################################################################\n",
        "    #                     TODO: Implement this function                      #\n",
        "    ##########################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    # Convert tokenized text data into numerical vectors\n",
        "    vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
        "    test_tokens = [word_tokenize(text) for text in test_df]\n",
        "    test_vec = vectorizer.transform(test_tokens)\n",
        "\n",
        "    # Predict the labels for the testing set\n",
        "    \n",
        "    ##########################################################################\n",
        "    #                            END OF YOUR CODE                            #\n",
        "    ##########################################################################\n",
        "    return "
      ],
      "metadata": {
        "id": "43T3JqK5a484"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 1 End\n"
      ],
      "metadata": {
        "id": "ue3xIDFGSXNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2 Start\n",
        "\n",
        "In this section you will write all details of your Method 2.\n",
        " \n",
        "You will have to enter multiple `code` and `text` cell.\n",
        "\n",
        "Your code should follow the standard ML pipeline\n",
        "\n",
        "\n",
        "*   Data reading\n",
        "*   Data clearning, if any\n",
        "*   Convert data to vector/tokenization/vectorization\n",
        "*   Model Declaration/Initialization/building\n",
        "*   Training and validation of the model using training and validation dataset \n",
        "*   Save the trained model\n",
        "*   Load and Test the model on testing set\n",
        "*   Save the output of the model\n",
        "\n",
        "You could add any other step(s) based on your method's requirement. \n",
        "\n",
        "After finishing the above, you need to usd splited data as defined in the assignment and then do the same for all 4 sets. Your code should not be copy-pasted 4 time, make use of `function`.\n"
      ],
      "metadata": {
        "id": "o5jNIHneSfzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Method 2 Code\n",
        "Your test code should be a stand alone code that must take `train_file`, `val_file`,  and `model_dir` as input. You could have other things as also input, but these three are must. You would load both files, and train using the `train_file` and validating using the `val_file`. You will `print` / `display`/ `plot` all performance metrics, loss(if available) and save the output model in the `model_dir`.\n",
        "\n",
        "Note that at the testing time, you need to use the same pre-processing and model. So, it would be good that you make those as seperate function/pipeline whichever it the best suited for your method. Don't copy-paste same code twice, make it a fucntion/class whichever is best. "
      ],
      "metadata": {
        "id": "zAkC0CWAc1BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_method2(train_file, val_file, model_dir):\n",
        "  \"\"\"\n",
        "     Takes train_file, val_file and model_dir as input.\n",
        "     It trained on the train_file datapoints, and validate on the val_file datapoints.\n",
        "     While training and validating, it print different evaluataion metrics and losses, wheverever necessary.\n",
        "     After finishing the training, it saved the best model in the model_dir.\n",
        "\n",
        "     ADD Other arguments, if needed.\n",
        "\n",
        "    Args:\n",
        "        train_file: Train file name\n",
        "        val_file: Validation file name\n",
        "        model_dir: Model output Directory\n",
        "        \n",
        "\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    ##########################################################################\n",
        "    #                     TODO: Implement this function                      #\n",
        "    ##########################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##########################################################################\n",
        "    #                            END OF YOUR CODE                            #\n",
        "    ##########################################################################\n",
        "    return "
      ],
      "metadata": {
        "id": "JA7vbLnoc0b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Method 2 Code\n",
        "Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`  "
      ],
      "metadata": {
        "id": "LVi2vGeZc5Fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_method2(test_file, model_file, output_dir):\n",
        "  \"\"\"\n",
        "     take test_file, model_file and output_dir as input.\n",
        "     It loads model and test of the examples in the test_file.\n",
        "     It prints different evaluation metrics, and saves the output in output directory\n",
        "\n",
        "     ADD Other arguments, if needed\n",
        "\n",
        "    Args:\n",
        "        test_file: test file name\n",
        "        model_file: model file name\n",
        "        output_dir: Output Directory\n",
        "        \n",
        "\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    ##########################################################################\n",
        "    #                     TODO: Implement this function                      #\n",
        "    ##########################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##########################################################################\n",
        "    #                            END OF YOUR CODE                            #\n",
        "    ##########################################################################\n",
        "    return "
      ],
      "metadata": {
        "id": "v477tmxOSlVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 2 End\n"
      ],
      "metadata": {
        "id": "4gNwyxXNSmVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Method/model Start"
      ],
      "metadata": {
        "id": "rmaJfJkVwSDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "LQTmVWa5wdD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Other Method/model End"
      ],
      "metadata": {
        "id": "7yMswIeAwYIf"
      }
    }
  ]
}